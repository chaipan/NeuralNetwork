AlexNet是2012年出现的。网络主要由5个卷积层和3个全连接层构成。
整个的网络计算过程如下：
输入数据为：[128, 227, 227, 3]


conv1：kernel=[11,11,3,96],stride=[1,4,4,1], padding="SAME"     结果数据：[128, 55, 55, 96]
relu1:                                                          结果数据：[128, 55, 55, 96]
LRN1：
pool1: kernel=[1,3,3,1], stride=[1,2,2,1], padding="SAME"       结果数据：[128, 27, 27, 96]
norm1:
由于使用两个单独的GPU进行计算，所以conv1的结构分成两个，分别是结果数据：[128, 55, 55, 48]，relu和pool分别单独对两个分开的数据做计算。

conv2：kernel=[5,5,96,256], stride=[1,1,1,1], padding=2         结果数据：[128, 27, 27, 256]
relu2:
pool2: kernel=[1,3,3,1], Stride=[1,2,2,1]   padding="SAME"      结果数据：[128, 13, 13, 256]
norm2:
在经过第一次卷积后数据分别由两个不同地GPU上计算，此时的数据在进行第二次卷积时同样分开计算，两个gpu之间并没有通信。

data flow diagram
conv3：kernel=[3,3,256,384]  stride=[1,1,1,1], padding=1        结果数据：[128, 13, 13, 384]
relu3:
第三个卷积层只做了卷积，没有池化处理和归一化处理，最后的输出数据时[128, 13, 13, 384]，同样每个GPU上分一半，此时两个GPU不通信。

data flow diagram
conv4: kernel=[3,3,384, 384], stride=[1,1,1,1] padding=1        结果数据：[128, 13, 13, 384]
relu4:
第四个卷积层只做了卷积，没有池化处理和归一化处理，最后的输出数据时[128, 13, 13, 384]，同样每个GPU上分一半，此时两个GPU不通信。

data flow diagram
conv5: kernel=[3,3,384,256] stride=[1,1,1,1],  padding=1        结果数据：[128, 13, 13, 256]
relu5:
pool5: kernel=[1,3,3,1] stride=[1,2,2,1]  padding="SAME"        数据结果: [128,6,6,256]

data flow diagram全连接层
fc6:  nodes=4096   w=[6*6*256, 4096]  b=[4096,1]                输出结果：[128, 4096]

data flow diagram全连接层
fc7:  nodes=4096   w=[6*6*256, 4096]  b=[4096,1]                输出结果：[128, 4096]
dropout7:

全连接层
fc8: nodes=1000    w=[4096, 1000]  b=[1000,1]                   输出结果：[128, 1000]
1000代表物体的分类类别

