
1、sigmoid
    sigmoid(x) = 1/(1 + e^-x)
    最开始在logistic回归中使用，logistic最开始使用的是跃阶函数，函数是不连续的，不利于运算。于是引进了sigmoid函数，sigmoid将整个实数轴映射在（0,1）
    区间上，很好的解决了函数的不连续问题。但是sigmoid函数存在梯度消失和梯度爆炸问题。


2、tanh函数
    tanh(x) = (1-e^-2x) / (1+e^-2x)
    tanh(x) = 2sigmoid(2x) - 1
    从式子上看tanh是将sigmoid在x轴上缩小一倍，在y轴上扩大一倍，然后将函数整个沿着y轴下移一单位而来。相比于s函数，
    tanh有所有的s函数的优点，并且具有更好的对称性。

3、ReLu(线性修正单元)
    f(x) = max(0, x)
    也常常表示为f(x) = {0 x<0;    x x>=0}

4、threshold（binary）
    f(x) = {0 x<0;  1 x>=0}

5、softmax
    f(x) = e^x / ∑e^x

6、maxout
    待定，没看懂

7、softplus
    f(x)=ln(1+e^x)
    是对relu函数的一种平滑逼近
    函数的导数就是sigmoid函数


激活函数通常有如下一些性质：
非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。
可微性： 当优化方法是基于梯度的时候，这个性质是必须的。
单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。
： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。
输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.